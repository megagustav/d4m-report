\begin{d4mbib}

@misc{speer2018,
      title={ConceptNet 5.5: An Open Multilingual Graph of General Knowledge}, 
      author={Robyn Speer and Joshua Chin and Catherine Havasi},
      year=2018,
      eprint={1612.03975},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/pdf/1612.03975.pdf}.
}

@paper{fournier1991, 
      AUTHOR={Susan Fournier},
      TITLE={Meaning-Based Framework For the Study of Consumer-Object Relations}, 
      JOURNAL={NA - Advances in Consumer Research,
      YEAR=1991,
      VOLUME= 18,
      ADDRESS={Provo, UT},
      EDITOR={Rebecca H. Holman and Michael R. Solomon},
      PAGES={736-742},
      URL={https://www.acrwebsite.org/volumes/7244/volumes/v18/NA-18}.
}

@misc{cuneo2020, title={Navigant Research recognizes four leaders in the development of automated driving systems}, url={https://web.archive.org/web/20200624181738/https://www.autonomousvehicletech.com/articles/2399-navigant-research-recognizes-four-leaders-in-the-development-of-automated-driving-systems}, journal={Autonomous Vehicle Technology RSS}, publisher={Autonomous Vehicle Technology}, author={Cuneo, Elisabeth}, year={2020}, month={Apr}}

@misc{korosec2015, title={Elon Musk Says Tesla Vehicles Will Drive Themselves in Two Years}, url={https://fortune.com/2015/12/21/elon-musk-interview/}, journal={Fortune}, publisher={Fortune}, author={Korosec, Kirsten}, year={2015}, month={Dec}}

@misc{etherington2020, title={Tesla has increased the price of its 'Full Self-Driving' option to \$10,000}, url={https://techcrunch.com/2020/10/29/tesla-has-increased-the-price-of-its-full-self-driving-option-to-10000/?guccounter=1&guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&guce_referrer_sig=AQAAAC_NZKFts-FE5_uWHLZd2vJU8Te9QZbwBJGum50ewLLJ3i1F_PbfqNQ7gN3aHaIm_1WhMS4OIwLM31sFcohUKnpB9qelOaEYxJ8jNwyB8V5PIrz4Gp4LICjr82yE1w8BZ8ulObA-m6ZSI7UhBxnEcH0q4QYr5U_X8iu8gv1osTQU}, journal={TechCrunch}, publisher={TechCrunch}, author={Etherington, Darrell}, year={2020}, month={Oct}}

@misc{reuters2020, title={Honda says will be first to mass produce level 3 autonomous cars}, url={https://www.reuters.com/article/honda-autonomous-level3-idUSKBN27R0M7}, journal={Reuters}, publisher={Thomson Reuters}, year={2020}, month={Nov}}

@misc{paukert2018, title={Why the 2019 Audi A8 won't get Level 3 Traffic Jam Pilot in the US}, url={https://www.cnet.com/roadshow/news/2019-audi-a8-level-3-traffic-jam-pilot-self-driving-automation-not-for-us/}, journal={Roadshow}, publisher={CNET}, author={Paukert, Chris}, year={2018}, month={May}}

@talk{giacomin2020,
Author={Joseph Giacomin},
Title={Human Centred Design of Autonomous Vehicles},
Year=2020,
Location={Politecnico di Milano}.
}

@manual{sae2018,
author={{SAE} International},
title={Taxonomy and Definitions for Terms Related to Driving Automation Systems for On-Road Motor Vehicles},
month={jun},
year={2018},
doi={https://doi.org/10.4271/J3016_201806},
url={https://doi.org/10.4271/J3016_201806},
abstract={This SAE Recommended Practice describes motor vehicle driving automation systems that perform part or all of the dynamic driving task (DDT) on a sustained basis. It provides a taxonomy with detailed definitions for six levels of driving automation, ranging from no driving automation (level 0) to full driving automation (level 5), in the context of motor vehicles (hereafter also referred to as “vehicle” or “vehicles”) and their operation on roadways. These level definitions, along with additional supporting terms and definitions provided herein, can be used to describe the full range of driving automation features equipped on motor vehicles in a functionally consistent and coherent manner. “On-road” refers to publicly accessible roadways (including parking areas and private campuses that permit public access) that collectively serve users of vehicles of all classes and driving automation levels (including no driving automation), as well as motorcyclists, pedal cyclists, and pedestrians.
The levels apply to the driving automation feature(s) that are engaged in any given instance of on-road operation of an equipped vehicle. As such, although a given vehicle may be equipped with a driving automation system that is capable of delivering multiple driving automation features that perform at different levels, the level of driving automation exhibited in any given instance is determined by the feature(s) that are engaged.
This document also refers to three primary actors in driving: the (human) user, the driving automation system, and other vehicle systems and components. These other vehicle systems and components (or the vehicle in general terms) do not include the driving automation system in this model, even though as a practical matter a driving automation system may actually share hardware and software components with other vehicle systems, such as a processing module(s) or operating code.
The levels of driving automation are defined by reference to the specific role played by each of the three primary actors in performance of the DDT and/or DDT fallback. “Role” in this context refers to the expected role of a given primary actor, based on the design of the driving automation system in question and not necessarily to the actual performance of a given primary actor. For example, a driver who fails to monitor the roadway during engagement of a level 1 adaptive cruise control (ACC) system still has the role of driver, even while s/he is neglecting it.
Active safety systems, such as electronic stability control and automated emergency braking, and certain types of driver assistance systems, such as lane keeping assistance, are excluded from the scope of this driving automation taxonomy because they do not perform part or all of the DDT on a sustained basis and, rather, merely provide momentary intervention during potentially hazardous situations. Due to the momentary nature of the actions of active safety systems, their intervention does not change or eliminate the role of the driver in performing part or all of the DDT, and thus are not considered to be driving automation.
It should, however, be noted that crash avoidance features, including intervention-type active safety systems, may be included in vehicles equipped with driving automation systems at any level. For Automated Driving System (ADS) features (i.e., levels 3-5) that perform the complete DDT, crash avoidance capability is part of ADS functionality.}
}

@article{mckinsey2016,
	AUTHOR={Martin Joerss, Jürgen Schröder, Florian Neuhaus, Christoph Klink, Florian Mann},
	title={Parcel delivery - The future of last mile},
    publisher={McKinsey and Company},
    year=2016,
    url={https://www.mckinsey.com/~/media/mckinsey/industries/travel%20transport%20and%20logistics/our%20insights/how%20customer%20demands%20are%20reshaping%20last%20mile%20delivery/parcel_delivery_the_future_of_last_mile.ashx}.
}

@misc{mcfarland2020, title={'I'm not drunk, it's my car:' Tesla's 'full self-driving' gets mixed reviews}, url={https://edition.cnn.com/2020/10/30/cars/tesla-full-self-driving/index.html}, journal={CNN}, publisher={Cable News Network}, author={McFarland, Matt}, year={2020}, month={Oct}}

@misc{haag2019, title={1.5 Million Packages a Day: The Internet Brings Chaos to N.Y. Streets}, url={https://www.nytimes.com/2019/10/27/nyregion/nyc-amazon-delivery.html}, journal={The New York Times}, publisher={The New York Times}, author={Haag, Matthew and Hu, Winnie}, year={2019}, month={Oct}}

@misc{monkeylearn2019, title={Natural Language Processing (NLP) – What Is NLP and How Does it Work?}, url={https://monkeylearn.com/natural-language-processing/}, journal={MonkeyLearn}} 

@book{newport2016,
      author={Cal Newport},
      title={Deep Work. Rules for Focused Success in a Distracted World},
      publisher={Hachette Book Group, Inc.},
      year=2016,
      isbn={978-1-4555-8666-0},
      url={https://ia800909.us.archive.org/13/items/DeepWorkRulesForFocusedSuccessInADistractedWorldPDFEngbbopdf.com/Deep%20Work%20Rules%20for%20Focused%20Success%20in%20a%20Distracted%20World%20PDF%20engbbopdf.com.pdf}.
}

@unknown{bosch2019,
author = {Natalie Bosch, Viktor Baumann},
year = {2019},
month = {10},
pages = {},
title = {Trust in Autonomous Cars}
}

@article{sweet2020,
author = {Sweet, Matthias and Laidlaw, Kailey},
year = {2020},
month = {10},
pages = {},
title = {No longer in the driver’s seat: How do affective motivations impact consumer interest in automated vehicles?},
volume = {47},
journal = {Transportation},
doi = {10.1007/s11116-019-10035-5}
}

@inproceedings{haeuslschmid2017,
author = {Häuslschmid, Renate and Bülow, Max and Pfleging, Bastian and Butz, Andreas},
year = {2017},
month = {03},
pages = {319-329},
title = {Supporting Trust in Autonomous Driving},
doi = {10.1145/3025171.3025198}
}



@website{eea2019,
author = {{European Environment Agency}},
year = 2019,
url = {https://www.eea.europa.eu/data-and-maps/indicators/transport-emissions-of-greenhouse-gases/transport-emissions-of-greenhouse-gases-12}.
}

@book{gobe2009,
author = {Marc Gobé},
year = 2009,
title = {Emotional branding: The new paradigm for connecting brands to people},
publisher = {Allworth Press},
address = {New York}.
}

@article{nonami2007,
author = {Nonami, Kenzo},
year = {2007},
month = {01},
pages = {120-128},
title = {Prospect and Recent Research and Development for Civil Use Autonomous Unmanned Aircraft as UAV and MAV},
volume = {1},
journal = {Journal of System Design and Dynamics},
doi = {10.1299/jsdd.1.120}
}

@article{bimbraw2015,
  title={Autonomous cars: Past, present and future a review of the developments in the last century, the present scenario and the expected future of autonomous vehicle technology},
  author={Keshav Bimbraw},
  journal={2015 12th International Conference on Informatics in Control, Automation and Robotics (ICINCO)},
  year={2015},
  volume={01},
  pages={191-198}
}

@inproceedings{eucom2015,
  title={Employment in the {EU} transport sector},
  author={{Directorate-General for Mobility and Transport (European Commission)}},
  year={2015}
}

@inproceedings{duffy2013,
  title={Sit, Stay, Drive: The Future of Autonomous Car Liability},
  author={Sophia N. Duffy and J. Hopkins},
  year={2013}
}

@article{diller2008,
author = {Diller, Steve and Shedroff, Nathan and Rhea, Darrel},
year = {2005},
month = {01},
pages = {},
title = {Making Meaning: How Successful Businesses Deliver Meaningful Customer Experiences},
isbn = {0321374096}
}

@article{crilly2008,
  title={Representing Artefacts as Media: Modelling the Relationship Between Designer Intent and Consumer Experience},
  author={Nathan Crilly and Anja Maier and P. Clarkson},
  journal={International Journal of Design},
  year={2008},
  volume={2},
  pages={15-27}
}

@inbook{csikszentmihalyi1981,
author = {Csikszentmihalyi, Mihaly and Rochberg-Halton, Eugene},
year = {1981},
month = {01},
pages = {},
title = {The Meaning of Things: Domestic Symbols and the Self},
volume = {12},
journal = {Contemporary Sociology},
doi = {10.2307/2067526}
}

@book{mitchell1997,
      author={Tom Mitchell},
      title={Machine Learning},
      publisher={McGraw Hill},
      year=1997,
      isbn={0070428077},
      url={http://www.cs.cmu.edu/~tom/mlbook.html}.
}

@article{altszyler2017,
      author={Edgar Altszyler and Sidarta Ribeiro and Mariano Sigman and Diego Fernández Slezak},
      title={The interpretation of dream meaning: Resolving ambiguity using Latent Semantic Analysis in a small corpus of text},
      journal={Consciousness and Cognition},
      volume=56;
      pages={178-187}
      year=2017,
      url={https://repositorio.ufrn.br/jspui/bitstream/123456789/24164/1/SidartaRibeiro_Theinterpretationofdream_2017.pdf}.
}

@paper{giacomin2017,
      AUTHOR={Joseph Giacomin},
      TITLE={What is Design for Meaning?},
      PUBLISHER={Human Centred Design Institute, Brunel University Uxbridge},
      ADDRESS={UB83PH, UK},
      YEAR=2017,
      URL={https://bura.brunel.ac.uk/bitstream/2438/15652/1/Fulltext.pdf}.
}

@misc{smilkov2016,
      title={Embedding Projector: Interactive Visualization and Interpretation of Embeddings}, 
      author={Daniel Smilkov and Nikhil Thorat and Charles Nicholson and Emily Reif and Fernanda B. Viégas and Martin Wattenberg},
      year={2016},
      eprint={1611.05469},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{mikolov2013,
      title={Efficient Estimation of Word Representations in Vector Space}, 
      author={Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
      year={2013},
      eprint={1301.3781},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@paper{turing1950,
      title={Computing Machinery and Intelligence},
      author={A. M. Turing},
      JOURNAL={Mind},
      VOLUME= 49,
      pages={433-460},
      year=1950,
      url={https://www.csee.umbc.edu/courses/471/papers/turing.pdf}.
}

@misc{trask2015,
      title={sense2vec - A Fast and Accurate Method for Word Sense Disambiguation In Neural Word Embeddings}, 
      author={Andrew Trask and Phil Michalak and John Liu},
      year={2015},
      eprint={1511.06388},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}



@inproceedings{reisinger2010, author = {Reisinger, Joseph and Mooney, Raymond J.}, title = {Multi-Prototype Vector-Space Models of Word Meaning}, year = {2010}, isbn = {1932432655}, publisher = {Association for Computational Linguistics}, address = {USA}, abstract = {Current vector-space models of lexical semantics create a single "prototype" vector to represent the meaning of a word. However, due to lexical ambiguity, encoding word meaning with a single vector is problematic. This paper presents a method that uses clustering to produce multiple "sense-specific" vectors for each word. This approach provides a context-dependent vector representation of word meaning that naturally accommodates homonymy and polysemy. Experimental comparisons to human judgements of semantic similarity for both isolated words as well as words in sentential contexts demonstrate the superiority of this approach over both prototype and exemplar based vector-space models.}, booktitle = {Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics}, pages = {109–117}, numpages = {9}, location = {Los Angeles, California}, series = {HLT '10} }

@book{committee1966language,
  title={Language and Machines: Computers in Translation and Linguistics; a Report},
  author={Committee, N.R.C.A.L.P.A.},
  lccn={66061843},
  series={National Research Council. Publication},
  url={https://books.google.de/books?id=Q0ErAAAAYAAJ},
  year={1966},
  publisher={National Academy of Sciences, National Research Council}
}

@article{weizenbaum1966,
author = {Weizenbaum, Joseph},
title = {ELIZA—a Computer Program for the Study of Natural Language Communication between Man and Machine},
year = {1966},
issue_date = {Jan. 1966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
issn = {0001-0782},
url = {https://doi.org/10.1145/365153.365168},
doi = {10.1145/365153.365168},
journal = {Commun. ACM},
month = jan,
pages = {36–45},
numpages = {10}
}

@inproceedings{maas2010,
  title={A Probabilistic Model for Semantic Word Vectors},
  author={Andrew L. Maas and A. Ng},
  year={2010}
}

@article{powers1984,
title = "Natural language the natural way",
journal = "Computer Compacts",
volume = "2",
number = "3",
pages = "100 - 109",
year = "1984",
issn = "0167-7136",
doi = "https://doi.org/10.1016/0167-7136(84)90088-X",
url = "http://www.sciencedirect.com/science/article/pii/016771368490088X",
author = "D.M.W. Powers",
abstract = "Traditional approaches to ‘Natural Language’ have concentrated on implementing specific grammars in the context of a particular word model or interlingual representation. They thus depend for their success on the adequacy of the linguistic theories used, on the validity of particularly limited syntactic and semantic databases of rules and volcabulary, and on the pragmatics of the underlying model. This research seeks an alternative approach to ‘Natural Language’, turning instead to the observations and theories of ‘Psycholinguistics’, ‘Neurolinguistics’ and, without prejudice, any other discipline, in an attempt to develop broad principles by which the responsibility of ‘learning’ the grammar and vocabulary of the language may be placed on the computer system itself. In order to place the onus on the machine, the system must be provided with sufficient interfaces, with sufficient generalization capabilities, and with a sufficiently interesting and accurate environment, so as to be able to learn ‘Natural Language’ the way we do, the way a baby does, ‘The Natural Way’."
}

@article{wordnet1995,
author = {Miller, George A.},
title = {WordNet: A Lexical Database for English},
year = {1995},
issue_date = {Nov. 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {11},
issn = {0001-0782},
url = {https://doi.org/10.1145/219717.219748},
doi = {10.1145/219717.219748},
abstract = {Because meaningful sentences are composed of meaningful words, any system that hopes to process natural languages as people do must have information about words and their meanings. This information is traditionally provided through dictionaries, and machine-readable dictionaries are now widely available. But dictionary entries evolved for the convenience of human readers, not for machines. WordNet1 provides a more effective combination of traditional lexicographic information and modern computing. WordNet is an online lexical database designed for use under program control. English nouns, verbs, adjectives, and adverbs are organized into sets of synonyms, each representing a lexicalized concept. Semantic relations link the synonym sets [4].},
journal = {Commun. ACM},
month = nov,
pages = {39–41},
numpages = {3}
}

@techreport{brown1979,
  added-at = {2008-02-29T17:14:20.000+0100},
  author = {Francis, W. N. and Kucera, H.},
  biburl = {https://www.bibsonomy.org/bibtex/260bb0c74c2ecced0632393e47eb64f48/sb3000},
  institution = {Department of Linguistics, Brown University, Providence, Rhode Island, US},
  interhash = {119c367841941ad1a8f0db35d9f1c0b9},
  intrahash = {60bb0c74c2ecced0632393e47eb64f48},
  keywords = {corpus linguistics text-mining},
  timestamp = {2008-02-29T17:14:20.000+0100},
  title = {Brown Corpus Manual},
  url = {http://icame.uib.no/brown/bcm.html},
  year = 1979
}

@inproceedings{chen2015,
    title = "Improving Distributed Representation of Word Sense via {W}ord{N}et Gloss Composition and Context Clustering",
    author = "Chen, Tao  and
      Xu, Ruifeng  and
      He, Yulan  and
      Wang, Xuan",
    booktitle = "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
    month = jul,
    year = "2015",
    address = "Beijing, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P15-2003",
    doi = "10.3115/v1/P15-2003",
    pages = "15-20",
}

@article{wattenberg2016,
  author = {Wattenberg, Martin and Viégas, Fernanda and Johnson, Ian},
  title = {How to Use t-SNE Effectively},
  journal = {Distill},
  year = {2016},
  url = {http://distill.pub/2016/misread-tsne},
  doi = {10.23915/distill.00002}
}

@inproceedings{horn2017,
    title = "Context encoders as a simple but powerful extension of word2vec",
    author = "Horn, Franziska",
    booktitle = "Proceedings of the 2nd Workshop on Representation Learning for {NLP}",
    month = aug,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W17-2602",
    doi = "10.18653/v1/W17-2602",
    pages = "10-14",
    abstract = "With a strikingly simple architecture and the ability to learn meaningful word embeddings efficiently from texts containing billions of words, word2vec remains one of the most popular neural language models used today. However, as only a single embedding is learned for every word in the vocabulary, the model fails to optimally represent words with multiple meanings and, additionally, it is not possible to create embeddings for new (out-of-vocabulary) words on the spot. Based on an intuitive interpretation of the continuous bag-of-words (CBOW) word2vec model{'}s negative sampling training objective in terms of predicting context based similarities, we motivate an extension of the model we call context encoders (ConEc). By multiplying the matrix of trained word2vec embeddings with a word{'}s average context vector, out-of-vocabulary (OOV) embeddings and representations for words with multiple meanings can be created based on the words{'} local contexts. The benefits of this approach are illustrated by using these word embeddings as features in the CoNLL 2003 named entity recognition (NER) task.",
}

@inproceedings{rothe2015,
    title = "{A}uto{E}xtend: Extending Word Embeddings to Embeddings for Synsets and Lexemes",
    author = {Rothe, Sascha  and
      Schütze, Hinrich},
    booktitle = "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = jul,
    year = "2015",
    address = "Beijing, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P15-1173",
    doi = "10.3115/v1/P15-1173",
    pages = "1793-1803",
}

@article{harris1954,
      author = {Zellig S. Harris},
      title = {Distributional Structure},
      journal = {WORD},
      volume = {10},
      number = {2-3},
      pages = {146-162},
      year  = {1954},
      publisher = {Routledge},
      doi = {10.1080/00437956.1954.11659520},
      URL = {https://doi.org/10.1080/00437956.1954.11659520},
      eprint = {https://doi.org/10.1080/00437956.1954.11659520}.
}

@inproceedings{alrfou2013,
    title ={Polyglot: Distributed Word Representations for Multilingual {NLP}},
    author = {Al-Rfou{'}, Rami,
      		  Perozzi, Bryan  and
      		  Skiena, Steven},
    booktitle = {Proceedings of the Seventeenth Conference on Computational Natural Language Learning},
    month = 8,
    year = 2013,
    address = {Sofia, Bulgaria},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/W13-3520},
    pages = {183-192},
}

@InProceedings{hutchins2004,
author={Hutchins, W. John},
editor={Frederking, Robert E.
and Taylor, Kathryn B.},
title={The Georgetown-IBM Experiment Demonstrated in January 1954},
booktitle={Machine Translation: From Real Users to Research},
year=2004,
publisher={Springer Berlin Heidelberg},
address={Berlin, Heidelberg},
pages={102--114},
abstract={The public demonstration of a Russian-English machine translation system in New York in January 1954 -- a collaboration of IBM and Georgetown University -- caused a great deal of public interest and much controversy. Although a small-scale experiment of just 250 words and six `grammar' rules it raised expectations of automatic systems capable of high quality translation in the near future. This paper describes the system, its background, its impact and its implications.},
isbn={978-3-540-30194-3},
url={https://link.springer.com/chapter/10.1007/978-3-540-30194-3_12}.
}

\end{d4mbib}