\begin{d4mbib}

@misc{speer2018,
      title={ConceptNet 5.5: An Open Multilingual Graph of General Knowledge}, 
      author={Robyn Speer and Joshua Chin and Catherine Havasi},
      year=2018,
      eprint={1612.03975},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/pdf/1612.03975.pdf}.
}

@paper{fournier1991, 
      AUTHOR={Susan Fournier},
      TITLE={Meaning-Based Framework For the Study of Consumer-Object Relations}, 
      JOURNAL={NA - Advances in Consumer Research,
      YEAR=1991,
      VOLUME= 18,
      ADDRESS={Provo, UT},
      EDITOR={Rebecca H. Holman and Michael R. Solomon},
      PAGES={736-742},
      URL={https://www.acrwebsite.org/volumes/7244/volumes/v18/NA-18}.
}

@misc{monkeylearn2019, title={Natural Language Processing (NLP) – What Is NLP and How Does it Work?}, url={https://monkeylearn.com/natural-language-processing/}, journal={MonkeyLearn}} 

@book{newport2016,
      author={Cal Newport},
      title={Deep Work. Rules for Focused Success in a Distracted World},
      publisher={Hachette Book Group, Inc.},
      year=2016,
      isbn={978-1-4555-8666-0},
      url={https://ia800909.us.archive.org/13/items/DeepWorkRulesForFocusedSuccessInADistractedWorldPDFEngbbopdf.com/Deep%20Work%20Rules%20for%20Focused%20Success%20in%20a%20Distracted%20World%20PDF%20engbbopdf.com.pdf}.
}

@book{mitchell1997,
      author={Tom Mitchell},
      title={Machine Learning},
      publisher={McGraw Hill},
      year=1997,
      isbn={0070428077},
      url={http://www.cs.cmu.edu/~tom/mlbook.html}.
}

@article{altszyler2017,
      author={Edgar Altszyler and Sidarta Ribeiro and Mariano Sigman and Diego Fernández Slezak},
      title={The interpretation of dream meaning: Resolving ambiguity using Latent Semantic Analysis in a small corpus of text},
      journal={Consciousness and Cognition},
      volume=56;
      pages={178-187}
      year=2017,
      url={https://repositorio.ufrn.br/jspui/bitstream/123456789/24164/1/SidartaRibeiro_Theinterpretationofdream_2017.pdf}.
}

@paper{giacomin2017,
      AUTHOR={Joseph Giacomin},
      TITLE={What is Design for Meaning?},
      PUBLISHER={Human Centred Design Institute, Brunel University Uxbridge},
      ADDRESS={UB83PH, UK},
      YEAR=2017,
      URL={https://bura.brunel.ac.uk/bitstream/2438/15652/1/Fulltext.pdf}.
}

@misc{smilkov2016,
      title={Embedding Projector: Interactive Visualization and Interpretation of Embeddings}, 
      author={Daniel Smilkov and Nikhil Thorat and Charles Nicholson and Emily Reif and Fernanda B. Viégas and Martin Wattenberg},
      year={2016},
      eprint={1611.05469},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{mikolov2013,
      title={Efficient Estimation of Word Representations in Vector Space}, 
      author={Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
      year={2013},
      eprint={1301.3781},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@paper{turing1950,
      title={Computing Machinery and Intelligence},
      author={A. M. Turing},
      JOURNAL={Mind},
      VOLUME= 49,
      pages={433-460},
      year=1950,
      url={https://www.csee.umbc.edu/courses/471/papers/turing.pdf}.
}

@misc{trask2015,
      title={sense2vec - A Fast and Accurate Method for Word Sense Disambiguation In Neural Word Embeddings}, 
      author={Andrew Trask and Phil Michalak and John Liu},
      year={2015},
      eprint={1511.06388},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}



@inproceedings{reisinger2010, author = {Reisinger, Joseph and Mooney, Raymond J.}, title = {Multi-Prototype Vector-Space Models of Word Meaning}, year = {2010}, isbn = {1932432655}, publisher = {Association for Computational Linguistics}, address = {USA}, abstract = {Current vector-space models of lexical semantics create a single "prototype" vector to represent the meaning of a word. However, due to lexical ambiguity, encoding word meaning with a single vector is problematic. This paper presents a method that uses clustering to produce multiple "sense-specific" vectors for each word. This approach provides a context-dependent vector representation of word meaning that naturally accommodates homonymy and polysemy. Experimental comparisons to human judgements of semantic similarity for both isolated words as well as words in sentential contexts demonstrate the superiority of this approach over both prototype and exemplar based vector-space models.}, booktitle = {Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics}, pages = {109–117}, numpages = {9}, location = {Los Angeles, California}, series = {HLT '10} }

@book{committee1966language,
  title={Language and Machines: Computers in Translation and Linguistics; a Report},
  author={Committee, N.R.C.A.L.P.A.},
  lccn={66061843},
  series={National Research Council. Publication},
  url={https://books.google.de/books?id=Q0ErAAAAYAAJ},
  year={1966},
  publisher={National Academy of Sciences, National Research Council}
}

@article{weizenbaum1966,
author = {Weizenbaum, Joseph},
title = {ELIZA—a Computer Program for the Study of Natural Language Communication between Man and Machine},
year = {1966},
issue_date = {Jan. 1966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
issn = {0001-0782},
url = {https://doi.org/10.1145/365153.365168},
doi = {10.1145/365153.365168},
journal = {Commun. ACM},
month = jan,
pages = {36–45},
numpages = {10}
}

@inproceedings{maas2010,
  title={A Probabilistic Model for Semantic Word Vectors},
  author={Andrew L. Maas and A. Ng},
  year={2010}
}

@article{powers1984,
title = "Natural language the natural way",
journal = "Computer Compacts",
volume = "2",
number = "3",
pages = "100 - 109",
year = "1984",
issn = "0167-7136",
doi = "https://doi.org/10.1016/0167-7136(84)90088-X",
url = "http://www.sciencedirect.com/science/article/pii/016771368490088X",
author = "D.M.W. Powers",
abstract = "Traditional approaches to ‘Natural Language’ have concentrated on implementing specific grammars in the context of a particular word model or interlingual representation. They thus depend for their success on the adequacy of the linguistic theories used, on the validity of particularly limited syntactic and semantic databases of rules and volcabulary, and on the pragmatics of the underlying model. This research seeks an alternative approach to ‘Natural Language’, turning instead to the observations and theories of ‘Psycholinguistics’, ‘Neurolinguistics’ and, without prejudice, any other discipline, in an attempt to develop broad principles by which the responsibility of ‘learning’ the grammar and vocabulary of the language may be placed on the computer system itself. In order to place the onus on the machine, the system must be provided with sufficient interfaces, with sufficient generalization capabilities, and with a sufficiently interesting and accurate environment, so as to be able to learn ‘Natural Language’ the way we do, the way a baby does, ‘The Natural Way’."
}

@article{wordnet1995,
author = {Miller, George A.},
title = {WordNet: A Lexical Database for English},
year = {1995},
issue_date = {Nov. 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {11},
issn = {0001-0782},
url = {https://doi.org/10.1145/219717.219748},
doi = {10.1145/219717.219748},
abstract = {Because meaningful sentences are composed of meaningful words, any system that hopes to process natural languages as people do must have information about words and their meanings. This information is traditionally provided through dictionaries, and machine-readable dictionaries are now widely available. But dictionary entries evolved for the convenience of human readers, not for machines. WordNet1 provides a more effective combination of traditional lexicographic information and modern computing. WordNet is an online lexical database designed for use under program control. English nouns, verbs, adjectives, and adverbs are organized into sets of synonyms, each representing a lexicalized concept. Semantic relations link the synonym sets [4].},
journal = {Commun. ACM},
month = nov,
pages = {39–41},
numpages = {3}
}

@techreport{brown1979,
  added-at = {2008-02-29T17:14:20.000+0100},
  author = {Francis, W. N. and Kucera, H.},
  biburl = {https://www.bibsonomy.org/bibtex/260bb0c74c2ecced0632393e47eb64f48/sb3000},
  institution = {Department of Linguistics, Brown University, Providence, Rhode Island, US},
  interhash = {119c367841941ad1a8f0db35d9f1c0b9},
  intrahash = {60bb0c74c2ecced0632393e47eb64f48},
  keywords = {corpus linguistics text-mining},
  timestamp = {2008-02-29T17:14:20.000+0100},
  title = {Brown Corpus Manual},
  url = {http://icame.uib.no/brown/bcm.html},
  year = 1979
}

@inproceedings{chen2015,
    title = "Improving Distributed Representation of Word Sense via {W}ord{N}et Gloss Composition and Context Clustering",
    author = "Chen, Tao  and
      Xu, Ruifeng  and
      He, Yulan  and
      Wang, Xuan",
    booktitle = "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
    month = jul,
    year = "2015",
    address = "Beijing, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P15-2003",
    doi = "10.3115/v1/P15-2003",
    pages = "15-20",
}

@article{wattenberg2016,
  author = {Wattenberg, Martin and Viégas, Fernanda and Johnson, Ian},
  title = {How to Use t-SNE Effectively},
  journal = {Distill},
  year = {2016},
  url = {http://distill.pub/2016/misread-tsne},
  doi = {10.23915/distill.00002}
}

@inproceedings{horn2017,
    title = "Context encoders as a simple but powerful extension of word2vec",
    author = "Horn, Franziska",
    booktitle = "Proceedings of the 2nd Workshop on Representation Learning for {NLP}",
    month = aug,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W17-2602",
    doi = "10.18653/v1/W17-2602",
    pages = "10-14",
    abstract = "With a strikingly simple architecture and the ability to learn meaningful word embeddings efficiently from texts containing billions of words, word2vec remains one of the most popular neural language models used today. However, as only a single embedding is learned for every word in the vocabulary, the model fails to optimally represent words with multiple meanings and, additionally, it is not possible to create embeddings for new (out-of-vocabulary) words on the spot. Based on an intuitive interpretation of the continuous bag-of-words (CBOW) word2vec model{'}s negative sampling training objective in terms of predicting context based similarities, we motivate an extension of the model we call context encoders (ConEc). By multiplying the matrix of trained word2vec embeddings with a word{'}s average context vector, out-of-vocabulary (OOV) embeddings and representations for words with multiple meanings can be created based on the words{'} local contexts. The benefits of this approach are illustrated by using these word embeddings as features in the CoNLL 2003 named entity recognition (NER) task.",
}

@inproceedings{rothe2015,
    title = "{A}uto{E}xtend: Extending Word Embeddings to Embeddings for Synsets and Lexemes",
    author = {Rothe, Sascha  and
      Schütze, Hinrich},
    booktitle = "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = jul,
    year = "2015",
    address = "Beijing, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P15-1173",
    doi = "10.3115/v1/P15-1173",
    pages = "1793-1803",
}

@article{harris1954,
      author = {Zellig S. Harris},
      title = {Distributional Structure},
      journal = {WORD},
      volume = {10},
      number = {2-3},
      pages = {146-162},
      year  = {1954},
      publisher = {Routledge},
      doi = {10.1080/00437956.1954.11659520},
      URL = {https://doi.org/10.1080/00437956.1954.11659520},
      eprint = {https://doi.org/10.1080/00437956.1954.11659520}.
}

@inproceedings{alrfou2013,
    title ={Polyglot: Distributed Word Representations for Multilingual {NLP}},
    author = {Al-Rfou{'}, Rami,
      		  Perozzi, Bryan  and
      		  Skiena, Steven},
    booktitle = {Proceedings of the Seventeenth Conference on Computational Natural Language Learning},
    month = 8,
    year = 2013,
    address = {Sofia, Bulgaria},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/W13-3520},
    pages = {183-192},
}

@InProceedings{hutchins2004,
author="Hutchins, W. John",
editor="Frederking, Robert E.
and Taylor, Kathryn B.",
title="The Georgetown-IBM Experiment Demonstrated in January 1954",
booktitle="Machine Translation: From Real Users to Research",
year="2004",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="102--114",
abstract="The public demonstration of a Russian-English machine translation system in New York in January 1954 -- a collaboration of IBM and Georgetown University -- caused a great deal of public interest and much controversy. Although a small-scale experiment of just 250 words and six `grammar' rules it raised expectations of automatic systems capable of high quality translation in the near future. This paper describes the system, its background, its impact and its implications.",
isbn="978-3-540-30194-3",
url="https://link.springer.com/chapter/10.1007/978-3-540-30194-3_12"
}

\end{d4mbib}